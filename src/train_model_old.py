"""
–°–∫—Ä–∏–ø—Ç –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –ò–ò –º–æ–¥–µ–ª–∏ –Ω–∞ –∏—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö
"""

import os
import json
import pandas as pd
import torch
from torch.utils.data import Dataset, DataLoader
from transformers import AutoTokenizer
from datasets import Dataset as HFDataset
import logging
from typing import List, Dict, Any
import argparse
from pathlib import Path

# –ò–º–ø–æ—Ä—Ç–∏—Ä—É–µ–º –Ω–∞—à—É –º–æ–¥–µ–ª—å –∏ –æ–±—Ä–∞–±–æ—Ç–∫—É –¥–∞–Ω–Ω—ã—Ö
from models.history_ai import HistoryAIModel
from data_processing import process_data_directory, get_supported_formats

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class HistoryDataset(Dataset):
    """–ö–ª–∞—Å—Å –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å –∏—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∏–º–∏ –¥–∞–Ω–Ω—ã–º–∏"""
    
    def __init__(self, texts: List[str], labels: List[int] = None, tokenizer=None, max_length: int = 512):
        """
        Args:
            texts: –°–ø–∏—Å–æ–∫ —Ç–µ–∫—Å—Ç–æ–≤
            labels: –°–ø–∏—Å–æ–∫ –º–µ—Ç–æ–∫ (–¥–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏)
            tokenizer: –¢–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä
            max_length: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
        """
        self.texts = texts
        self.labels = labels
        self.tokenizer = tokenizer
        self.max_length = max_length
        self.is_classification = labels is not None
    
    def __len__(self):
        return len(self.texts)
    
    def __getitem__(self, idx):
        text = str(self.texts[idx])
        
        # –¢–æ–∫–µ–Ω–∏–∑–∏—Ä—É–µ–º —Ç–µ–∫—Å—Ç
        encoding = self.tokenizer(
            text,
            truncation=True,
            padding='max_length',
            max_length=self.max_length,
            return_tensors='pt'
        )
        
        result = {
            'input_ids': encoding['input_ids'].flatten(),
            'attention_mask': encoding['attention_mask'].flatten()
        }
        
        # –î–æ–±–∞–≤–ª—è–µ–º –º–µ—Ç–∫–∏ –¥–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏
        if self.is_classification:
            result['labels'] = torch.tensor(self.labels[idx], dtype=torch.long)
        
        return result

def load_historical_data(data_path: str) -> Dict[str, Any]:
    """
    –ó–∞–≥—Ä—É–∂–∞–µ—Ç –∏—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∏–µ –¥–∞–Ω–Ω—ã–µ –∏–∑ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö —Ñ–æ—Ä–º–∞—Ç–æ–≤
    
    Args:
        data_path: –ü—É—Ç—å –∫ —Ñ–∞–π–ª—É –∏–ª–∏ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ —Å –¥–∞–Ω–Ω—ã–º–∏
    
    Returns:
        –°–ª–æ–≤–∞—Ä—å —Å –¥–∞–Ω–Ω—ã–º–∏
    """
    data_path = Path(data_path)
    
    if not data_path.exists():
        raise FileNotFoundError(f"–ü—É—Ç—å {data_path} –Ω–µ –Ω–∞–π–¥–µ–Ω")
    
    logger.info(f"–ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞–Ω–Ω—ã–µ –∏–∑ {data_path}")
    
    # –ï—Å–ª–∏ —ç—Ç–æ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è, –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –≤—Å–µ —Ñ–∞–π–ª—ã
    if data_path.is_dir():
        logger.info("–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –≤—Å–µ —Ñ–∞–π–ª—ã –≤ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏")
        texts = process_data_directory(data_path)
        data = [{'text': text} for text in texts]
    else:
        # –ï—Å–ª–∏ —ç—Ç–æ —Ñ–∞–π–ª, –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –µ–≥–æ
        if data_path.suffix.lower() == '.json':
            # –ó–∞–≥—Ä—É–∂–∞–µ–º JSON —Ñ–∞–π–ª
            with open(data_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
            logger.info(f"–ó–∞–≥—Ä—É–∂–µ–Ω JSON —Ñ–∞–π–ª —Å {len(data)} –∑–∞–ø–∏—Å—è–º–∏")
        else:
            # –û–±—ã—á–Ω—ã–π —Ñ–∞–π–ª
            from data_processing import load_file
            text = load_file(data_path)
            if text:
                data = [{'text': text}]
            else:
                data = []
    
    logger.info(f"–ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(data)} –∑–∞–ø–∏—Å–µ–π")
    return data

def prepare_training_data(data: List[Dict], task_type: str = "generation") -> tuple:
    """
    –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ—Ç –¥–∞–Ω–Ω—ã–µ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è
    
    Args:
        data: –°–ø–∏—Å–æ–∫ —Å–ª–æ–≤–∞—Ä–µ–π —Å –¥–∞–Ω–Ω—ã–º–∏
        task_type: –¢–∏–ø –∑–∞–¥–∞—á–∏ ('generation', 'classification')
    
    Returns:
        –ö–æ—Ä—Ç–µ–∂ —Å –ø–æ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω–Ω—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏
    """
    texts = []
    labels = []
    
    for item in data:
        if isinstance(item, dict):
            # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ç–µ–∫—Å—Ç
            if 'text' in item:
                texts.append(item['text'])
            elif 'content' in item:
                texts.append(item['content'])
            elif 'description' in item:
                texts.append(item['description'])
            else:
                # –ë–µ—Ä–µ–º –ø–µ—Ä–≤–æ–µ —Å—Ç—Ä–æ–∫–æ–≤–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ
                for key, value in item.items():
                    if isinstance(value, str) and len(value) > 10:
                        texts.append(value)
                        break
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º –º–µ—Ç–∫–∏ –¥–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏
            if task_type == "classification":
                if 'label' in item:
                    labels.append(item['label'])
                elif 'category' in item:
                    labels.append(item['category'])
                elif 'class' in item:
                    labels.append(item['class'])
                else:
                    labels.append(0)  # –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é
    
    if task_type == "classification" and not labels:
        logger.warning("–ú–µ—Ç–∫–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã, —Å–æ–∑–¥–∞–µ–º —Ñ–∏–∫—Ç–∏–≤–Ω—ã–µ –º–µ—Ç–∫–∏")
        labels = [0] * len(texts)
    
    return texts, labels if task_type == "classification" else None

def create_sample_data():
    """–°–æ–∑–¥–∞–µ—Ç –ø—Ä–∏–º–µ—Ä –∏—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è"""
    sample_data = [
        {
            "text": "–í 1812 –≥–æ–¥—É –ù–∞–ø–æ–ª–µ–æ–Ω –ë–æ–Ω–∞–ø–∞—Ä—Ç –≤—Ç–æ—Ä–≥—Å—è –≤ –†–æ—Å—Å–∏—é —Å –∞—Ä–º–∏–µ–π –≤ 600 —Ç—ã—Å—è—á —á–µ–ª–æ–≤–µ–∫. –≠—Ç–æ —Å–æ–±—ã—Ç–∏–µ —Å—Ç–∞–ª–æ –Ω–∞—á–∞–ª–æ–º –û—Ç–µ—á–µ—Å—Ç–≤–µ–Ω–Ω–æ–π –≤–æ–π–Ω—ã 1812 –≥–æ–¥–∞.",
            "category": "–≤–æ–π–Ω–∞"
        },
        {
            "text": "–ü–µ—Ç—Ä I –í–µ–ª–∏–∫–∏–π –ø—Ä–æ–≤–µ–ª –º–∞—Å—à—Ç–∞–±–Ω—ã–µ —Ä–µ—Ñ–æ—Ä–º—ã –≤ –†–æ—Å—Å–∏–∏, –≤–∫–ª—é—á–∞—è —Å–æ–∑–¥–∞–Ω–∏–µ —Ä–µ–≥—É–ª—è—Ä–Ω–æ–π –∞—Ä–º–∏–∏ –∏ —Ñ–ª–æ—Ç–∞, –∞ —Ç–∞–∫–∂–µ –æ—Å–Ω–æ–≤–∞–Ω–∏–µ –°–∞–Ω–∫—Ç-–ü–µ—Ç–µ—Ä–±—É—Ä–≥–∞ –≤ 1703 –≥–æ–¥—É.",
            "category": "—Ä–µ—Ñ–æ—Ä–º—ã"
        },
        {
            "text": "–í 988 –≥–æ–¥—É –∫–Ω—è–∑—å –í–ª–∞–¥–∏–º–∏—Ä –∫—Ä–µ—Å—Ç–∏–ª –†—É—Å—å, –ø—Ä–∏–Ω—è–≤ —Ö—Ä–∏—Å—Ç–∏–∞–Ω—Å—Ç–≤–æ –∏–∑ –í–∏–∑–∞–Ω—Ç–∏–∏. –≠—Ç–æ —Å–æ–±—ã—Ç–∏–µ –æ–∫–∞–∑–∞–ª–æ –æ–≥—Ä–æ–º–Ω–æ–µ –≤–ª–∏—è–Ω–∏–µ –Ω–∞ —Ä–∞–∑–≤–∏—Ç–∏–µ —Ä—É—Å—Å–∫–æ–π –∫—É–ª—å—Ç—É—Ä—ã.",
            "category": "—Ä–µ–ª–∏–≥–∏—è"
        },
        {
            "text": "–ö—É–ª–∏–∫–æ–≤—Å–∫–∞—è –±–∏—Ç–≤–∞ 1380 –≥–æ–¥–∞ —Å—Ç–∞–ª–∞ –ø–µ—Ä–µ–ª–æ–º–Ω—ã–º –º–æ–º–µ–Ω—Ç–æ–º –≤ –±–æ—Ä—å–±–µ —Ä—É—Å—Å–∫–∏—Ö –∫–Ω—è–∂–µ—Å—Ç–≤ –ø—Ä–æ—Ç–∏–≤ –º–æ–Ω–≥–æ–ª–æ-—Ç–∞—Ç–∞—Ä—Å–∫–æ–≥–æ –∏–≥–∞.",
            "category": "–≤–æ–π–Ω–∞"
        },
        {
            "text": "–ò–≤–∞–Ω IV –ì—Ä–æ–∑–Ω—ã–π –±—ã–ª –ø–µ—Ä–≤—ã–º —Ä—É—Å—Å–∫–∏–º —Ü–∞—Ä–µ–º, –∫–æ—Ä–æ–Ω–æ–≤–∞–Ω–Ω—ã–º –≤ 1547 –≥–æ–¥—É. –ï–≥–æ –ø—Ä–∞–≤–ª–µ–Ω–∏–µ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏–∑–æ–≤–∞–ª–æ—Å—å —Ü–µ–Ω—Ç—Ä–∞–ª–∏–∑–∞—Ü–∏–µ–π –≤–ª–∞—Å—Ç–∏ –∏ –æ–ø—Ä–∏—á–Ω–∏–Ω–æ–π.",
            "category": "–ø–æ–ª–∏—Ç–∏–∫–∞"
        },
        {
            "text": "–°–º—É—Ç–Ω–æ–µ –≤—Ä–µ–º—è (1598-1613) –±—ã–ª–æ –ø–µ—Ä–∏–æ–¥–æ–º –ø–æ–ª–∏—Ç–∏—á–µ—Å–∫–æ–≥–æ –∫—Ä–∏–∑–∏—Å–∞ –≤ –†–æ—Å—Å–∏–∏, –∑–∞–≤–µ—Ä—à–∏–≤—à–∏–º—Å—è –∏–∑–±—Ä–∞–Ω–∏–µ–º –Ω–∞ –ø—Ä–µ—Å—Ç–æ–ª –¥–∏–Ω–∞—Å—Ç–∏–∏ –†–æ–º–∞–Ω–æ–≤—ã—Ö.",
            "category": "–ø–æ–ª–∏—Ç–∏–∫–∞"
        },
        {
            "text": "–ê–ª–µ–∫—Å–∞–Ω–¥—Ä II –æ—Ç–º–µ–Ω–∏–ª –∫—Ä–µ–ø–æ—Å—Ç–Ω–æ–µ –ø—Ä–∞–≤–æ –≤ 1861 –≥–æ–¥—É, —á—Ç–æ —Å—Ç–∞–ª–æ –≤–∞–∂–Ω—ã–º —à–∞–≥–æ–º –≤ –º–æ–¥–µ—Ä–Ω–∏–∑–∞—Ü–∏–∏ –†–æ—Å—Å–∏–∏.",
            "category": "—Ä–µ—Ñ–æ—Ä–º—ã"
        },
        {
            "text": "–û–∫—Ç—è–±—Ä—å—Å–∫–∞—è —Ä–µ–≤–æ–ª—é—Ü–∏—è 1917 –≥–æ–¥–∞ –ø—Ä–∏–≤–µ–ª–∞ –∫ —Å–≤–µ—Ä–∂–µ–Ω–∏—é –í—Ä–µ–º–µ–Ω–Ω–æ–≥–æ –ø—Ä–∞–≤–∏—Ç–µ–ª—å—Å—Ç–≤–∞ –∏ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏—é —Å–æ–≤–µ—Ç—Å–∫–æ–π –≤–ª–∞—Å—Ç–∏ –≤ –†–æ—Å—Å–∏–∏.",
            "category": "—Ä–µ–≤–æ–ª—é—Ü–∏—è"
        }
    ]
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ø—Ä–∏–º–µ—Ä –¥–∞–Ω–Ω—ã—Ö
    os.makedirs("data/raw", exist_ok=True)
    with open("data/raw/sample_history_data.json", "w", encoding="utf-8") as f:
        json.dump(sample_data, f, ensure_ascii=False, indent=2)
    
    logger.info("–°–æ–∑–¥–∞–Ω —Ñ–∞–π–ª —Å –ø—Ä–∏–º–µ—Ä–æ–º –∏—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö: data/raw/sample_history_data.json")
    return sample_data

def train_model(data_path: str = "data/raw", task_type: str = "generation", num_epochs: int = 3, model_name: str = "microsoft/DialoGPT-medium"):
    """
    –û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏
    
    Args:
        data_path: –ü—É—Ç—å –∫ –¥–∞–Ω–Ω—ã–º (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é data/raw)
        task_type: –¢–∏–ø –∑–∞–¥–∞—á–∏
        num_epochs: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —ç–ø–æ—Ö
        model_name: –ù–∞–∑–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏
    """
    try:
        # –°–æ–∑–¥–∞–µ–º –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏
        os.makedirs("models", exist_ok=True)
        os.makedirs("logs", exist_ok=True)
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞–Ω–Ω—ã–µ
        if data_path == "sample":
            data = create_sample_data()
        else:
            data = load_historical_data(data_path)
            
            # –ï—Å–ª–∏ –ø–∞–ø–∫–∞ data/raw –ø—É—Å—Ç–∞, –≤—ã–¥–∞–µ–º –æ—à–∏–±–∫—É
            if not data and data_path == "data/raw":
                raise ValueError(
                    "–ü–∞–ø–∫–∞ data/raw –ø—É—Å—Ç–∞! –î–æ–±–∞–≤—å—Ç–µ —Ñ–∞–π–ª—ã –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏.\n"
                    "–ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–µ —Ñ–æ—Ä–º–∞—Ç—ã: TXT, CSV, JSON, PDF, DOC, DOCX, DJVU"
                )
        
        # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –¥–∞–Ω–Ω—ã–µ
        texts, labels = prepare_training_data(data, task_type)
        
        if not texts:
            raise ValueError("–ù–µ —É–¥–∞–ª–æ—Å—å –∏–∑–≤–ª–µ—á—å —Ç–µ–∫—Å—Ç—ã –∏–∑ –¥–∞–Ω–Ω—ã—Ö")
        
        logger.info(f"–ü–æ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω–æ {len(texts)} —Ç–µ–∫—Å—Ç–æ–≤ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è")
        
        # –°–æ–∑–¥–∞–µ–º –º–æ–¥–µ–ª—å
        ai_model = HistoryAIModel(model_name=model_name)
        ai_model.load_model(task_type)
        
        # –°–æ–∑–¥–∞–µ–º –¥–∞—Ç–∞—Å–µ—Ç
        if task_type == "classification":
            dataset = HistoryDataset(texts, labels, ai_model.tokenizer)
        else:
            dataset = HistoryDataset(texts, None, ai_model.tokenizer)
        
        # –î–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Ç–µ–∫—Å—Ç–∞ —Å–æ–∑–¥–∞–µ–º –ø—Ä–∞–≤–∏–ª—å–Ω—ã–π –¥–∞—Ç–∞—Å–µ—Ç
        if task_type == "generation":
            # –°–æ–∑–¥–∞–µ–º –¥–∞—Ç–∞—Å–µ—Ç –¥–ª—è —è–∑—ã–∫–æ–≤–æ–≥–æ –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏—è
            texts_for_lm = []
            for text in texts:
                # –î–æ–±–∞–≤–ª—è–µ–º —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã–µ —Ç–æ–∫–µ–Ω—ã
                formatted_text = f"<|startoftext|>{text}<|endoftext|>"
                texts_for_lm.append(formatted_text)
            
            # –¢–æ–∫–µ–Ω–∏–∑–∏—Ä—É–µ–º –≤—Å–µ —Ç–µ–∫—Å—Ç—ã
            tokenized_texts = []
            for text in texts_for_lm:
                tokens = ai_model.tokenizer(
                    text,
                    truncation=True,
                    padding='max_length',
                    max_length=128,
                    return_tensors='pt'
                )
                tokenized_texts.append({
                    'input_ids': tokens['input_ids'].flatten(),
                    'attention_mask': tokens['attention_mask'].flatten(),
                    'labels': tokens['input_ids'].flatten()  # –î–ª—è —è–∑—ã–∫–æ–≤–æ–≥–æ –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏—è labels = input_ids
                })
            
            hf_dataset = HFDataset.from_list([
                {
                    'input_ids': item['input_ids'].tolist(),
                    'attention_mask': item['attention_mask'].tolist(),
                    'labels': item['labels'].tolist()
                }
                for item in tokenized_texts
            ])
        else:
            # –î–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ –∏—Å–ø–æ–ª—å–∑—É–µ–º —Å—Ç–∞—Ä—ã–π –ø–æ–¥—Ö–æ–¥
            hf_dataset = HFDataset.from_list([
                {
                    'input_ids': item['input_ids'].tolist(),
                    'attention_mask': item['attention_mask'].tolist(),
                    **({'labels': item['labels'].item()} if 'labels' in item else {})
                }
                for item in dataset
            ])
        
        # –†–∞–∑–¥–µ–ª—è–µ–º –Ω–∞ train/eval
        train_size = int(0.8 * len(hf_dataset))
        train_dataset = hf_dataset.select(range(train_size))
        eval_dataset = hf_dataset.select(range(train_size, len(hf_dataset)))
        
        # –û–±—É—á–∞–µ–º –º–æ–¥–µ–ª—å
        ai_model.train(train_dataset, eval_dataset, num_epochs)
        
        logger.info("–û–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ —É—Å–ø–µ—à–Ω–æ!")
        
        # –¢–µ—Å—Ç–∏—Ä—É–µ–º –º–æ–¥–µ–ª—å
        if task_type == "generation":
            test_prompt = "–í –∏—Å—Ç–æ—Ä–∏–∏ –†–æ—Å—Å–∏–∏ –≤–∞–∂–Ω—ã–º —Å–æ–±—ã—Ç–∏–µ–º –±—ã–ª–æ:"
            generated = ai_model.generate_text(test_prompt, max_length=100)
            print(f"\nüß™ –¢–µ—Å—Ç –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏:")
            print(f"–ü—Ä–æ–º–ø—Ç: {test_prompt}")
            print(f"–†–µ–∑—É–ª—å—Ç–∞—Ç: {generated}")
        
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—É—á–µ–Ω–∏–∏: {e}")
        raise

def main():
    """–ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è"""
    parser = argparse.ArgumentParser(description="–û–±—É—á–µ–Ω–∏–µ –ò–ò –º–æ–¥–µ–ª–∏ –¥–ª—è –∏–∑—É—á–µ–Ω–∏—è –∏—Å—Ç–æ—Ä–∏–∏")
    parser.add_argument("--data", type=str, default="data/raw", 
                       help="–ü—É—Ç—å –∫ –ø–∞–ø–∫–µ —Å –¥–∞–Ω–Ω—ã–º–∏ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é data/raw) –∏–ª–∏ 'sample' –¥–ª—è —Ç–µ—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö")
    parser.add_argument("--task", type=str, default="generation", 
                       choices=["generation", "classification"],
                       help="–¢–∏–ø –∑–∞–¥–∞—á–∏")
    parser.add_argument("--epochs", type=int, default=3, 
                       help="–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —ç–ø–æ—Ö –æ–±—É—á–µ–Ω–∏—è")
    parser.add_argument("--model", type=str, default="microsoft/DialoGPT-medium",
                       help="–ù–∞–∑–≤–∞–Ω–∏–µ –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏")
    
    args = parser.parse_args()
    
    print("üöÄ –ù–∞—á–∏–Ω–∞–µ–º –æ–±—É—á–µ–Ω–∏–µ –ò–ò –º–æ–¥–µ–ª–∏ –¥–ª—è –∏–∑—É—á–µ–Ω–∏—è –∏—Å—Ç–æ—Ä–∏–∏")
    print(f"üìä –î–∞–Ω–Ω—ã–µ: {args.data}")
    print(f"üéØ –ó–∞–¥–∞—á–∞: {args.task}")
    print(f"üîÑ –≠–ø–æ—Ö–∏: {args.epochs}")
    print(f"ü§ñ –ú–æ–¥–µ–ª—å: {args.model}")
    print("=" * 60)
    
    train_model(args.data, args.task, args.epochs, args.model)

if __name__ == "__main__":
    main()
