#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
–ò–Ω–∫—Ä–µ–º–µ–Ω—Ç–∞–ª—å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –¥–∞–Ω–Ω—ã—Ö —Å –æ—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏–µ–º –∏–∑—É—á–µ–Ω–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤
"""

import os
import sys
import json
from pathlib import Path
import logging
from typing import List, Dict, Optional, Union

# –î–æ–±–∞–≤–ª—è–µ–º –ø—É—Ç—å –∫ –º–æ–¥—É–ª—è–º
sys.path.append(str(Path(__file__).parent))

from data_processing import load_file, process_data_directory
from file_tracker import FileTracker

logger = logging.getLogger(__name__)

class IncrementalDataProcessor:
    """
    –ö–ª–∞—Å—Å –¥–ª—è –∏–Ω–∫—Ä–µ–º–µ–Ω—Ç–∞–ª—å–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏ –¥–∞–Ω–Ω—ã—Ö
    """
    
    def __init__(self, tracking_file: str = "data/processed/learned_files.json"):
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä–∞
        
        Args:
            tracking_file: –ü—É—Ç—å –∫ —Ñ–∞–π–ª—É –æ—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏—è
        """
        self.tracker = FileTracker(tracking_file)
        self.processed_data_file = Path("data/processed/pdf_history_data.json")
        self.processed_data_file.parent.mkdir(parents=True, exist_ok=True)
    
    def process_new_files(self, data_path: Union[Path, str, List[Path]]) -> List[Dict]:
        """
        –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç —Ç–æ–ª—å–∫–æ –Ω–æ–≤—ã–µ —Ñ–∞–π–ª—ã
        
        Args:
            data_path: –ü—É—Ç—å –∫ –¥–∞–Ω–Ω—ã–º (—Ñ–∞–π–ª, –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –∏–ª–∏ —Å–ø–∏—Å–æ–∫ —Ñ–∞–π–ª–æ–≤)
            
        Returns:
            –°–ø–∏—Å–æ–∫ –Ω–æ–≤—ã—Ö –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
        """
        logger.info(f"–ù–∞—á–∏–Ω–∞–µ–º –∏–Ω–∫—Ä–µ–º–µ–Ω—Ç–∞–ª—å–Ω—É—é –æ–±—Ä–∞–±–æ—Ç–∫—É: {data_path}")
        
        # –ü–æ–ª—É—á–∞–µ–º —Å–ø–∏—Å–æ–∫ –≤—Å–µ—Ö —Ñ–∞–π–ª–æ–≤
        if isinstance(data_path, list):
            # –ï—Å–ª–∏ –ø–µ—Ä–µ–¥–∞–Ω —Å–ø–∏—Å–æ–∫ —Ñ–∞–π–ª–æ–≤
            all_files = data_path
        elif isinstance(data_path, (str, Path)):
            data_path = Path(data_path)
            if data_path.is_dir():
                all_files = list(data_path.glob("*"))
                # –§–∏–ª—å—Ç—Ä—É–µ–º —Ç–æ–ª—å–∫–æ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–µ —Ñ–æ—Ä–º–∞—Ç—ã
                supported_extensions = {'.pdf', '.txt', '.doc', '.docx', '.djvu', '.fb2'}
                all_files = [f for f in all_files if f.suffix.lower() in supported_extensions]
            else:
                all_files = [data_path]
        else:
            raise ValueError(f"–ù–µ–ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–π —Ç–∏–ø data_path: {type(data_path)}")
        
        # –ü–æ–ª—É—á–∞–µ–º —Ç–æ–ª—å–∫–æ –Ω–æ–≤—ã–µ —Ñ–∞–π–ª—ã
        new_files = self.tracker.get_new_files(all_files)
        
        if not new_files:
            logger.info("–ù–µ—Ç –Ω–æ–≤—ã—Ö —Ñ–∞–π–ª–æ–≤ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏")
            return []
        
        logger.info(f"–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º {len(new_files)} –Ω–æ–≤—ã—Ö —Ñ–∞–π–ª–æ–≤")
        
        new_data = []
        for file_path in new_files:
            try:
                logger.info(f"–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –Ω–æ–≤—ã–π —Ñ–∞–π–ª: {file_path.name}")
                
                # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Ñ–∞–π–ª
                text = load_file(file_path)
                
                # –£–ª—É—á—à–µ–Ω–Ω–∞—è –≤–∞–ª–∏–¥–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞
                if text:
                    # –£–±–∏—Ä–∞–µ–º –ª–∏—à–Ω–∏–µ –ø—Ä–æ–±–µ–ª—ã –∏ –ø—Ä–æ–≤–µ—Ä—è–µ–º –¥–ª–∏–Ω—É
                    cleaned_text = text.strip()
                    
                    if len(cleaned_text) > 200:  # –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º –º–∏–Ω–∏–º–∞–ª—å–Ω—É—é –¥–ª–∏–Ω—É
                        # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –æ—á–∏—Å—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞
                        from data_processing import clean_text
                        final_text = clean_text(cleaned_text)
                        
                        if final_text and len(final_text.strip()) > 100:
                            # –°–æ–∑–¥–∞–µ–º –∑–∞–ø–∏—Å—å –¥–∞–Ω–Ω—ã—Ö
                            data_entry = {
                                'filename': file_path.name,
                                'text': final_text,
                                'original_length': len(text),
                                'processed_length': len(final_text),
                                'file_path': str(file_path)
                            }
                            new_data.append(data_entry)
                            
                            # –û—Ç–º–µ—á–∞–µ–º —Ñ–∞–π–ª –∫–∞–∫ –∏–∑—É—á–µ–Ω–Ω—ã–π
                            self.tracker.mark_file_as_learned(
                                file_path, 
                                len(final_text), 
                                "incremental_processing"
                            )
                            
                            logger.info(f"‚úÖ –§–∞–π–ª {file_path.name} –æ–±—Ä–∞–±–æ—Ç–∞–Ω: {len(final_text)} —Å–∏–º–≤–æ–ª–æ–≤ (–∏—Å—Ö–æ–¥–Ω–æ: {len(text)})")
                        else:
                            logger.warning(f"‚ö†Ô∏è –§–∞–π–ª {file_path.name} —Å–æ–¥–µ—Ä–∂–∏—Ç –º–∞–ª–æ —Ç–µ–∫—Å—Ç–∞ –ø–æ—Å–ª–µ –æ—á–∏—Å—Ç–∫–∏: {len(final_text) if final_text else 0} —Å–∏–º–≤–æ–ª–æ–≤")
                    else:
                        logger.warning(f"‚ö†Ô∏è –§–∞–π–ª {file_path.name} —Å–æ–¥–µ—Ä–∂–∏—Ç –º–∞–ª–æ —Ç–µ–∫—Å—Ç–∞: {len(cleaned_text)} —Å–∏–º–≤–æ–ª–æ–≤")
                else:
                    logger.warning(f"‚ö†Ô∏è –§–∞–π–ª {file_path.name} –Ω–µ —Å–æ–¥–µ—Ä–∂–∏—Ç —Ç–µ–∫—Å—Ç–∞ –∏–ª–∏ –Ω–µ —É–¥–∞–ª–æ—Å—å –µ–≥–æ –∏–∑–≤–ª–µ—á—å")
                    
            except Exception as e:
                logger.error(f"‚ùå –û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ñ–∞–π–ª–∞ {file_path.name}: {e}")
        
        # –û–±–Ω–æ–≤–ª—è–µ–º –æ—Å–Ω–æ–≤–Ω–æ–π —Ñ–∞–π–ª –¥–∞–Ω–Ω—ã—Ö
        if new_data:
            self._update_processed_data_file(new_data)
        
        return new_data
    
    def _update_processed_data_file(self, new_data: List[Dict]):
        """
        –û–±–Ω–æ–≤–ª—è–µ—Ç –æ—Å–Ω–æ–≤–Ω–æ–π —Ñ–∞–π–ª –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
        
        Args:
            new_data: –ù–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ –¥–ª—è –¥–æ–±–∞–≤–ª–µ–Ω–∏—è
        """
        try:
            # –ó–∞–≥—Ä—É–∂–∞–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –¥–∞–Ω–Ω—ã–µ
            existing_data = []
            if self.processed_data_file.exists():
                with open(self.processed_data_file, 'r', encoding='utf-8') as f:
                    existing_data = json.load(f)
            
            # –î–æ–±–∞–≤–ª—è–µ–º –Ω–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ
            existing_data.extend(new_data)
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ–±–Ω–æ–≤–ª–µ–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
            with open(self.processed_data_file, 'w', encoding='utf-8') as f:
                json.dump(existing_data, f, ensure_ascii=False, indent=2)
            
            logger.info(f"–§–∞–π–ª {self.processed_data_file} –æ–±–Ω–æ–≤–ª–µ–Ω: –¥–æ–±–∞–≤–ª–µ–Ω–æ {len(new_data)} –∑–∞–ø–∏—Å–µ–π")
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è —Ñ–∞–π–ª–∞ –¥–∞–Ω–Ω—ã—Ö: {e}")
    
    def get_processing_stats(self) -> Dict:
        """
        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –æ–±—Ä–∞–±–æ—Ç–∫–∏
        
        Returns:
            –°–ª–æ–≤–∞—Ä—å —Å–æ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–æ–π
        """
        learned_info = self.tracker.get_learned_files_info()
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞–Ω–Ω—ã–µ –∏–∑ –æ—Å–Ω–æ–≤–Ω–æ–≥–æ —Ñ–∞–π–ª–∞
        total_records = 0
        if self.processed_data_file.exists():
            try:
                with open(self.processed_data_file, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    total_records = len(data)
            except:
                pass
        
        return {
            "learned_files": learned_info["total_files"],
            "total_text_length": learned_info["total_text_length"],
            "total_records": total_records,
            "tracking_file": str(self.tracker.tracking_file)
        }
    
    def reset_learning(self):
        """
        –°–±—Ä–∞—Å—ã–≤–∞–µ—Ç –¥–∞–Ω–Ω—ã–µ –æ–±—É—á–µ–Ω–∏—è
        """
        self.tracker.reset_tracking()
        if self.processed_data_file.exists():
            self.processed_data_file.unlink()
        logger.info("–î–∞–Ω–Ω—ã–µ –æ–±—É—á–µ–Ω–∏—è —Å–±—Ä–æ—à–µ–Ω—ã")

def main():
    """
    –û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è
    """
    import argparse
    
    parser = argparse.ArgumentParser(description='–ò–Ω–∫—Ä–µ–º–µ–Ω—Ç–∞–ª—å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –¥–∞–Ω–Ω—ã—Ö')
    parser.add_argument('--data', type=str, required=True, help='–ü—É—Ç—å –∫ –¥–∞–Ω–Ω—ã–º')
    parser.add_argument('--reset', action='store_true', help='–°–±—Ä–æ—Å–∏—Ç—å –¥–∞–Ω–Ω—ã–µ –æ–±—É—á–µ–Ω–∏—è')
    parser.add_argument('--stats', action='store_true', help='–ü–æ–∫–∞–∑–∞—Ç—å —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É')
    
    args = parser.parse_args()
    
    # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
    logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
    
    processor = IncrementalDataProcessor()
    
    if args.reset:
        processor.reset_learning()
        return
    
    if args.stats:
        stats = processor.get_processing_stats()
        print(f"üìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏:")
        print(f"  –ò–∑—É—á–µ–Ω–æ —Ñ–∞–π–ª–æ–≤: {stats['learned_files']}")
        print(f"  –û–±—â–∏–π –æ–±—ä–µ–º —Ç–µ–∫—Å—Ç–∞: {stats['total_text_length']:,} —Å–∏–º–≤–æ–ª–æ–≤")
        print(f"  –ó–∞–ø–∏—Å–µ–π –≤ –¥–∞–Ω–Ω—ã—Ö: {stats['total_records']}")
        return
    
    data_path = Path(args.data)
    if not data_path.exists():
        logger.error(f"–ü—É—Ç—å –Ω–µ –Ω–∞–π–¥–µ–Ω: {data_path}")
        return
    
    # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –Ω–æ–≤—ã–µ —Ñ–∞–π–ª—ã
    new_data = processor.process_new_files(data_path)
    
    if new_data:
        print(f"‚úÖ –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ {len(new_data)} –Ω–æ–≤—ã—Ö —Ñ–∞–π–ª–æ–≤")
        for item in new_data:
            print(f"  - {item['filename']}: {item['processed_length']:,} —Å–∏–º–≤–æ–ª–æ–≤")
    else:
        print("‚ÑπÔ∏è –ù–µ—Ç –Ω–æ–≤—ã—Ö —Ñ–∞–π–ª–æ–≤ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏")

if __name__ == "__main__":
    main()
