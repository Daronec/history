#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
–ò–Ω–∫—Ä–µ–º–µ–Ω—Ç–∞–ª—å–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ —Å –æ—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏–µ–º –∏–∑—É—á–µ–Ω–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤
"""

import argparse
import sys
import json
from pathlib import Path
import logging
import torch
from typing import List
from datasets import Dataset as HFDataset

# –î–æ–±–∞–≤–ª—è–µ–º –ø—É—Ç—å –∫ –º–æ–¥—É–ª—è–º
sys.path.append(str(Path(__file__).parent))

from models.history_ai import HistoryAIModel
from incremental_data_processing import IncrementalDataProcessor

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

def load_historical_data(data_path: str) -> list:
    """
    –ó–∞–≥—Ä—É–∂–∞–µ—Ç –∏—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∏–µ –¥–∞–Ω–Ω—ã–µ –∏–∑ —Ñ–∞–π–ª–∞
    
    Args:
        data_path: –ü—É—Ç—å –∫ –¥–∞–Ω–Ω—ã–º
        
    Returns:
        –°–ø–∏—Å–æ–∫ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è
    """
    data_file = Path(data_path)
    
    if not data_file.exists():
        raise ValueError(f"–§–∞–π–ª –¥–∞–Ω–Ω—ã—Ö –Ω–µ –Ω–∞–π–¥–µ–Ω: {data_path}")
    
    logger.info(f"–ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞–Ω–Ω—ã–µ –∏–∑ {data_path}")
    
    with open(data_file, 'r', encoding='utf-8') as f:
        data = json.load(f)
    
    logger.info(f"–ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(data)} –∑–∞–ø–∏—Å–µ–π")
    return data

def extract_texts_from_data(data: list) -> list:
    """
    –ò–∑–≤–ª–µ–∫–∞–µ—Ç —Ç–µ–∫—Å—Ç—ã –∏–∑ –¥–∞–Ω–Ω—ã—Ö
    
    Args:
        data: –°–ø–∏—Å–æ–∫ –¥–∞–Ω–Ω—ã—Ö
        
    Returns:
        –°–ø–∏—Å–æ–∫ —Ç–µ–∫—Å—Ç–æ–≤
    """
    texts = []
    for item in data:
        if isinstance(item, dict) and 'text' in item:
            texts.append(item['text'])
        elif isinstance(item, str):
            texts.append(item)
    
    logger.info(f"–ü–æ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω–æ {len(texts)} —Ç–µ–∫—Å—Ç–æ–≤ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è")
    return texts

def create_dataset_for_training(texts: List[str], tokenizer, max_length: int = 512):
    """
    –°–æ–∑–¥–∞–µ—Ç –¥–∞—Ç–∞—Å–µ—Ç –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏
    
    Args:
        texts: –°–ø–∏—Å–æ–∫ —Ç–µ–∫—Å—Ç–æ–≤
        tokenizer: –¢–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä
        max_length: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
        
    Returns:
        –î–∞—Ç–∞—Å–µ—Ç –¥–ª—è –æ–±—É—á–µ–Ω–∏—è
    """
    logger.info("–°–æ–∑–¥–∞–µ–º –¥–∞—Ç–∞—Å–µ—Ç –¥–ª—è –æ–±—É—á–µ–Ω–∏—è...")
    
    # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –¥–∞–Ω–Ω—ã–µ
    formatted_texts = []
    for text in texts:
        # –î–æ–±–∞–≤–ª—è–µ–º —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã–µ —Ç–æ–∫–µ–Ω—ã
        formatted_text = f"<|startoftext|>{text}<|endoftext|>"
        formatted_texts.append(formatted_text)
    
    # –¢–æ–∫–µ–Ω–∏–∑–∏—Ä—É–µ–º
    tokenized_data = []
    for text in formatted_texts:
        tokens = tokenizer(
            text,
            truncation=True,
            padding='max_length',
            max_length=max_length,
            return_tensors="pt"
        )
        tokenized_data.append({
            "input_ids": tokens["input_ids"].squeeze().tolist(),
            "attention_mask": tokens["attention_mask"].squeeze().tolist(),
            "labels": tokens["input_ids"].squeeze().tolist()  # –î–ª—è —è–∑—ã–∫–æ–≤–æ–≥–æ –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏—è
        })
    
    # –°–æ–∑–¥–∞–µ–º HuggingFace –¥–∞—Ç–∞—Å–µ—Ç
    hf_dataset = HFDataset.from_list(tokenized_data)
    
    # –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º —Ñ–æ—Ä–º–∞—Ç –¥–ª—è PyTorch
    hf_dataset.set_format("torch")
    
    logger.info(f"–°–æ–∑–¥–∞–Ω –¥–∞—Ç–∞—Å–µ—Ç —Å {len(hf_dataset)} –ø—Ä–∏–º–µ—Ä–∞–º–∏")
    return hf_dataset

def train_model_incremental(data_path: str, task: str = "generation", epochs: int = 1, 
                          model_name: str = "ai-forever/rugpt3small_based_on_gpt2"):
    """
    –ò–Ω–∫—Ä–µ–º–µ–Ω—Ç–∞–ª—å–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏
    
    Args:
        data_path: –ü—É—Ç—å –∫ –¥–∞–Ω–Ω—ã–º
        task: –¢–∏–ø –∑–∞–¥–∞—á–∏
        epochs: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —ç–ø–æ—Ö
        model_name: –ù–∞–∑–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏
    """
    print("üöÄ –ù–∞—á–∏–Ω–∞–µ–º –∏–Ω–∫—Ä–µ–º–µ–Ω—Ç–∞–ª—å–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ –ò–ò –º–æ–¥–µ–ª–∏ –¥–ª—è –∏–∑—É—á–µ–Ω–∏—è –∏—Å—Ç–æ—Ä–∏–∏")
    print(f"üìä –î–∞–Ω–Ω—ã–µ: {data_path}")
    print(f"üéØ –ó–∞–¥–∞—á–∞: {task}")
    print(f"üîÑ –≠–ø–æ—Ö–∏: {epochs}")
    print(f"ü§ñ –ú–æ–¥–µ–ª—å: {model_name}")
    print("=" * 60)
    
    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä –¥–∞–Ω–Ω—ã—Ö
    processor = IncrementalDataProcessor()
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
    stats = processor.get_processing_stats()
    print(f"üìà –¢–µ–∫—É—â–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞:")
    print(f"  –ò–∑—É—á–µ–Ω–æ —Ñ–∞–π–ª–æ–≤: {stats['learned_files']}")
    print(f"  –û–±—â–∏–π –æ–±—ä–µ–º —Ç–µ–∫—Å—Ç–∞: {stats['total_text_length']:,} —Å–∏–º–≤–æ–ª–æ–≤")
    print(f"  –ó–∞–ø–∏—Å–µ–π –≤ –¥–∞–Ω–Ω—ã—Ö: {stats['total_records']}")
    print()
    
    # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –Ω–æ–≤—ã–µ —Ñ–∞–π–ª—ã
    data_path_obj = Path(data_path)
    new_data = processor.process_new_files(data_path_obj)
    
    if not new_data:
        print("‚ÑπÔ∏è –ù–µ—Ç –Ω–æ–≤—ã—Ö —Ñ–∞–π–ª–æ–≤ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è")
        print("üîÑ –ü—Ä–æ–¥–æ–ª–∂–∞–µ–º –æ–±—É—á–µ–Ω–∏–µ –Ω–∞ –≤—Å–µ—Ö —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö –¥–∞–Ω–Ω—ã—Ö...")
    else:
        print(f"üÜï –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ {len(new_data)} –Ω–æ–≤—ã—Ö —Ñ–∞–π–ª–æ–≤:")
        for item in new_data:
            print(f"  - {item['filename']}: {item['processed_length']:,} —Å–∏–º–≤–æ–ª–æ–≤")
        print()
    
    # –ó–∞–≥—Ä—É–∂–∞–µ–º –≤—Å–µ –¥–∞–Ω–Ω—ã–µ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è
    processed_data_file = Path("data/processed/pdf_history_data.json")
    if not processed_data_file.exists():
        raise ValueError("–§–∞–π–ª –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –Ω–µ –Ω–∞–π–¥–µ–Ω")
    
    data = load_historical_data(str(processed_data_file))
    texts = extract_texts_from_data(data)
    
    if not texts:
        raise ValueError("–ù–µ —É–¥–∞–ª–æ—Å—å –∏–∑–≤–ª–µ—á—å —Ç–µ–∫—Å—Ç—ã –∏–∑ –¥–∞–Ω–Ω—ã—Ö")
    
    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –º–æ–¥–µ–ª—å
    logger.info(f"–ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å {model_name} –¥–ª—è –∑–∞–¥–∞—á–∏: {task}")
    model = HistoryAIModel(model_name=model_name)
    model.load_model(task_type=task)
    
    # –°–æ–∑–¥–∞–µ–º –¥–∞—Ç–∞—Å–µ—Ç –¥–ª—è –æ–±—É—á–µ–Ω–∏—è
    logger.info("–°–æ–∑–¥–∞–µ–º –¥–∞—Ç–∞—Å–µ—Ç –¥–ª—è –æ–±—É—á–µ–Ω–∏—è...")
    train_dataset = create_dataset_for_training(texts, model.tokenizer)
    
    # –û–±—É—á–∞–µ–º –º–æ–¥–µ–ª—å
    logger.info("–ù–∞—á–∏–Ω–∞–µ–º –æ–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏...")
    model.train(train_dataset, num_epochs=epochs)
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–æ–¥–µ–ª—å
    model.save_model("models/history_ai_trained")
    logger.info("–û–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ. –ú–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞.")
    
    print("‚úÖ –ò–Ω–∫—Ä–µ–º–µ–Ω—Ç–∞–ª—å–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ —É—Å–ø–µ—à–Ω–æ!")
    
    # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –æ–±–Ω–æ–≤–ª–µ–Ω–Ω—É—é —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
    stats = processor.get_processing_stats()
    print(f"\nüìà –û–±–Ω–æ–≤–ª–µ–Ω–Ω–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞:")
    print(f"  –ò–∑—É—á–µ–Ω–æ —Ñ–∞–π–ª–æ–≤: {stats['learned_files']}")
    print(f"  –û–±—â–∏–π –æ–±—ä–µ–º —Ç–µ–∫—Å—Ç–∞: {stats['total_text_length']:,} —Å–∏–º–≤–æ–ª–æ–≤")
    print(f"  –ó–∞–ø–∏—Å–µ–π –≤ –¥–∞–Ω–Ω—ã—Ö: {stats['total_records']}")
    
    # –¢–µ—Å—Ç–∏—Ä—É–µ–º –º–æ–¥–µ–ª—å
    print("\nüß™ –¢–µ—Å—Ç –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏:")
    test_prompt = "–í –∏—Å—Ç–æ—Ä–∏–∏ –†–æ—Å—Å–∏–∏ –≤–∞–∂–Ω—ã–º —Å–æ–±—ã—Ç–∏–µ–º –±—ã–ª–æ:"
    result = model.generate_text(prompt=test_prompt, max_length=100, temperature=0.7)
    print(f"–ü—Ä–æ–º–ø—Ç: {test_prompt}")
    print(f"–†–µ–∑—É–ª—å—Ç–∞—Ç: {result}")

def main():
    parser = argparse.ArgumentParser(description='–ò–Ω–∫—Ä–µ–º–µ–Ω—Ç–∞–ª—å–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ –ò–ò –º–æ–¥–µ–ª–∏')
    parser.add_argument('--data', type=str, help='–ü—É—Ç—å –∫ –¥–∞–Ω–Ω—ã–º –¥–ª—è –æ–±—É—á–µ–Ω–∏—è')
    parser.add_argument('--task', type=str, default='generation', help='–¢–∏–ø –∑–∞–¥–∞—á–∏ (generation)')
    parser.add_argument('--epochs', type=int, default=1, help='–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —ç–ø–æ—Ö –æ–±—É—á–µ–Ω–∏—è')
    parser.add_argument('--model', type=str, default='ai-forever/rugpt3small_based_on_gpt2', 
                       help='–ù–∞–∑–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è')
    parser.add_argument('--reset', action='store_true', help='–°–±—Ä–æ—Å–∏—Ç—å –¥–∞–Ω–Ω—ã–µ –æ–±—É—á–µ–Ω–∏—è')
    parser.add_argument('--stats', action='store_true', help='–ü–æ–∫–∞–∑–∞—Ç—å —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É')
    
    args = parser.parse_args()
    
    if args.reset:
        processor = IncrementalDataProcessor()
        processor.reset_learning()
        print("‚úÖ –î–∞–Ω–Ω—ã–µ –æ–±—É—á–µ–Ω–∏—è —Å–±—Ä–æ—à–µ–Ω—ã")
        return
    
    if args.stats:
        processor = IncrementalDataProcessor()
        stats = processor.get_processing_stats()
        print(f"üìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –æ–±—É—á–µ–Ω–∏—è:")
        print(f"  –ò–∑—É—á–µ–Ω–æ —Ñ–∞–π–ª–æ–≤: {stats['learned_files']}")
        print(f"  –û–±—â–∏–π –æ–±—ä–µ–º —Ç–µ–∫—Å—Ç–∞: {stats['total_text_length']:,} —Å–∏–º–≤–æ–ª–æ–≤")
        print(f"  –ó–∞–ø–∏—Å–µ–π –≤ –¥–∞–Ω–Ω—ã—Ö: {stats['total_records']}")
        print(f"  –§–∞–π–ª –æ—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏—è: {stats['tracking_file']}")
        return
    
    if not args.data:
        print("‚ùå –ù–µ–æ–±—Ö–æ–¥–∏–º–æ —É–∫–∞–∑–∞—Ç—å –ø—É—Ç—å –∫ –¥–∞–Ω–Ω—ã–º —Å –ø–æ–º–æ—â—å—é --data")
        return
    
    try:
        train_model_incremental(
            data_path=args.data,
            task=args.task,
            epochs=args.epochs,
            model_name=args.model
        )
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—É—á–µ–Ω–∏–∏: {e}")
        sys.exit(1)

if __name__ == "__main__":
    main()
