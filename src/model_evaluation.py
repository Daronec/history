#!/usr/bin/env python3
"""
–ú–æ–¥—É–ª—å –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞ –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏
–ü—Ä–æ–≤–µ—Ä—è–µ—Ç, –Ω–∞—Å–∫–æ–ª—å–∫–æ —Ö–æ—Ä–æ—à–æ –º–æ–¥–µ–ª—å –∏–∑—É—á–∏–ª–∞ –º–∞—Ç–µ—Ä–∏–∞–ª—ã
"""

import os
import json
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM
import logging
from pathlib import Path
from typing import List, Dict, Any, Tuple
import random

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class ModelEvaluator:
    """–ö–ª–∞—Å—Å –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞ –æ–±—É—á–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏"""
    
    def __init__(self, model_path: str, model_type: str = "english"):
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –æ—Ü–µ–Ω—â–∏–∫–∞ –º–æ–¥–µ–ª–∏
        
        Args:
            model_path: –ü—É—Ç—å –∫ –æ–±—É—á–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏
            model_type: –¢–∏–ø –º–æ–¥–µ–ª–∏ ('english' –∏–ª–∏ 'russian')
        """
        self.model_path = Path(model_path)
        self.model_type = model_type
        self.tokenizer = None
        self.model = None
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        
    def load_model(self):
        """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –æ–±—É—á–µ–Ω–Ω—É—é –º–æ–¥–µ–ª—å"""
        try:
            logger.info(f"–ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å –∏–∑ {self.model_path}")
            self.tokenizer = AutoTokenizer.from_pretrained(self.model_path)
            self.model = AutoModelForCausalLM.from_pretrained(self.model_path)
            self.model.to(self.device)
            self.model.eval()
            logger.info("–ú–æ–¥–µ–ª—å —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω–∞")
            return True
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏: {e}")
            return False
    
    def generate_text(self, prompt: str, max_length: int = 200, temperature: float = 0.7) -> str:
        """
        –ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç —Ç–µ–∫—Å—Ç –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø—Ä–æ–º–ø—Ç–∞
        
        Args:
            prompt: –¢–µ–∫—Å—Ç-–∑–∞–ø—Ä–æ—Å
            max_length: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞ –æ—Ç–≤–µ—Ç–∞
            temperature: –¢–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ (0.1-1.0)
        
        Returns:
            –°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç
        """
        try:
            # –¢–æ–∫–µ–Ω–∏–∑–∏—Ä—É–µ–º –ø—Ä–æ–º–ø—Ç
            inputs = self.tokenizer.encode(prompt, return_tensors="pt").to(self.device)
            
            # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º —Ç–µ–∫—Å—Ç
            with torch.no_grad():
                outputs = self.model.generate(
                    inputs,
                    max_length=max_length,
                    temperature=temperature,
                    do_sample=True,
                    pad_token_id=self.tokenizer.eos_token_id,
                    eos_token_id=self.tokenizer.eos_token_id,
                    no_repeat_ngram_size=2
                )
            
            # –î–µ–∫–æ–¥–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç
            generated_text = self.tokenizer.decode(outputs[0], skip_special_tokens=True)
            
            # –£–±–∏—Ä–∞–µ–º –∏—Å—Ö–æ–¥–Ω—ã–π –ø—Ä–æ–º–ø—Ç –∏–∑ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞
            if generated_text.startswith(prompt):
                generated_text = generated_text[len(prompt):].strip()
            
            return generated_text
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Ç–µ–∫—Å—Ç–∞: {e}")
            return ""
    
    def test_knowledge_retention(self, test_questions: List[Dict[str, str]]) -> Dict[str, Any]:
        """
        –¢–µ—Å—Ç–∏—Ä—É–µ—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∑–Ω–∞–Ω–∏–π –º–æ–¥–µ–ª–∏
        
        Args:
            test_questions: –°–ø–∏—Å–æ–∫ —Ç–µ—Å—Ç–æ–≤—ã—Ö –≤–æ–ø—Ä–æ—Å–æ–≤ –∏ –æ–∂–∏–¥–∞–µ–º—ã—Ö –æ—Ç–≤–µ—Ç–æ–≤
        
        Returns:
            –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è
        """
        if not self.model:
            logger.error("–ú–æ–¥–µ–ª—å –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω–∞")
            return {}
        
        results = {
            "total_questions": len(test_questions),
            "correct_answers": 0,
            "partial_answers": 0,
            "incorrect_answers": 0,
            "detailed_results": []
        }
        
        logger.info(f"–ù–∞—á–∏–Ω–∞–µ–º —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –Ω–∞ {len(test_questions)} –≤–æ–ø—Ä–æ—Å–∞—Ö")
        
        for i, question_data in enumerate(test_questions):
            question = question_data["question"]
            expected_keywords = question_data.get("keywords", [])
            expected_answer = question_data.get("expected", "")
            
            logger.info(f"–í–æ–ø—Ä–æ—Å {i+1}: {question}")
            
            # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –æ—Ç–≤–µ—Ç
            generated_answer = self.generate_text(question, max_length=150)
            
            # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –∫–∞—á–µ—Å—Ç–≤–æ –æ—Ç–≤–µ—Ç–∞
            quality_score = self._analyze_answer_quality(
                generated_answer, expected_keywords, expected_answer
            )
            
            result = {
                "question": question,
                "generated_answer": generated_answer,
                "expected_keywords": expected_keywords,
                "quality_score": quality_score,
                "status": "correct" if quality_score >= 0.7 else "partial" if quality_score >= 0.4 else "incorrect"
            }
            
            results["detailed_results"].append(result)
            
            # –û–±–Ω–æ–≤–ª—è–µ–º —Å—á–µ—Ç—á–∏–∫–∏
            if quality_score >= 0.7:
                results["correct_answers"] += 1
            elif quality_score >= 0.4:
                results["partial_answers"] += 1
            else:
                results["incorrect_answers"] += 1
            
            logger.info(f"–ö–∞—á–µ—Å—Ç–≤–æ –æ—Ç–≤–µ—Ç–∞: {quality_score:.2f} ({result['status']})")
        
        # –í—ã—á–∏—Å–ª—è–µ–º –æ–±—â–∏–π –ø—Ä–æ—Ü–µ–Ω—Ç
        results["accuracy_percentage"] = (results["correct_answers"] / results["total_questions"]) * 100
        results["partial_accuracy_percentage"] = ((results["correct_answers"] + results["partial_answers"]) / results["total_questions"]) * 100
        
        return results
    
    def _analyze_answer_quality(self, generated_answer: str, expected_keywords: List[str], expected_answer: str) -> float:
        """
        –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –∫–∞—á–µ—Å—Ç–≤–æ —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ –æ—Ç–≤–µ—Ç–∞
        
        Args:
            generated_answer: –°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –æ—Ç–≤–µ—Ç
            expected_keywords: –û–∂–∏–¥–∞–µ–º—ã–µ –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞
            expected_answer: –û–∂–∏–¥–∞–µ–º—ã–π –æ—Ç–≤–µ—Ç
        
        Returns:
            –û—Ü–µ–Ω–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞ (0.0 - 1.0)
        """
        if not generated_answer:
            return 0.0
        
        score = 0.0
        generated_lower = generated_answer.lower()
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤
        if expected_keywords:
            keyword_matches = 0
            for keyword in expected_keywords:
                if keyword.lower() in generated_lower:
                    keyword_matches += 1
            
            keyword_score = keyword_matches / len(expected_keywords)
            score += keyword_score * 0.6  # 60% –≤–µ—Å–∞ –¥–ª—è –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥–ª–∏–Ω—É –æ—Ç–≤–µ—Ç–∞ (–¥–æ–ª–∂–µ–Ω –±—ã—Ç—å –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –ø–æ–¥—Ä–æ–±–Ω—ã–º)
        length_score = min(len(generated_answer) / 100, 1.0)  # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –∫ 100 —Å–∏–º–≤–æ–ª–∞–º
        score += length_score * 0.2  # 20% –≤–µ—Å–∞ –¥–ª—è –¥–ª–∏–Ω—ã
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å–≤—è–∑–Ω–æ—Å—Ç—å (–æ—Ç—Å—É—Ç—Å—Ç–≤–∏–µ –ø–æ–≤—Ç–æ—Ä–µ–Ω–∏–π)
        words = generated_answer.split()
        if len(words) > 0:
            unique_words = len(set(words))
            coherence_score = unique_words / len(words)
            score += coherence_score * 0.2  # 20% –≤–µ—Å–∞ –¥–ª—è —Å–≤—è–∑–Ω–æ—Å—Ç–∏
        
        return min(score, 1.0)
    
    def create_test_questions(self, data_path: str) -> List[Dict[str, str]]:
        """
        –°–æ–∑–¥–∞–µ—Ç —Ç–µ—Å—Ç–æ–≤—ã–µ –≤–æ–ø—Ä–æ—Å—ã –Ω–∞ –æ—Å–Ω–æ–≤–µ –¥–∞–Ω–Ω—ã—Ö
        
        Args:
            data_path: –ü—É—Ç—å –∫ –¥–∞–Ω–Ω—ã–º –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –≤–æ–ø—Ä–æ—Å–æ–≤
        
        Returns:
            –°–ø–∏—Å–æ–∫ —Ç–µ—Å—Ç–æ–≤—ã—Ö –≤–æ–ø—Ä–æ—Å–æ–≤
        """
        # –ò–º–ø–æ—Ä—Ç–∏—Ä—É–µ–º —Ñ—É–Ω–∫—Ü–∏—é –æ–±—Ä–∞–±–æ—Ç–∫–∏ –¥–∞–Ω–Ω—ã—Ö
        import sys
        sys.path.append(str(Path(__file__).parent))
        from data_processing import process_data_directory, load_file
        
        data_path = Path(data_path)
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞–Ω–Ω—ã–µ
        if data_path.is_dir():
            texts = process_data_directory(data_path)
        else:
            text = load_file(data_path)
            texts = [text] if text else []
        
        if not texts:
            logger.warning("–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –¥–∞–Ω–Ω—ã–µ –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è —Ç–µ—Å—Ç–æ–≤—ã—Ö –≤–æ–ø—Ä–æ—Å–æ–≤")
            return []
        
        # –°–æ–∑–¥–∞–µ–º —Ç–µ—Å—Ç–æ–≤—ã–µ –≤–æ–ø—Ä–æ—Å—ã –Ω–∞ –æ—Å–Ω–æ–≤–µ —Å–æ–¥–µ—Ä–∂–∏–º–æ–≥–æ
        test_questions = []
        
        # –ë–∞–∑–æ–≤—ã–µ –≤–æ–ø—Ä–æ—Å—ã –¥–ª—è –∏—Å—Ç–æ—Ä–∏–∏ –†–æ—Å—Å–∏–∏
        basic_questions = [
            {
                "question": "–ö–æ–≥–¥–∞ –ø—Ä–æ–∏–∑–æ—à–ª–∞ –ö—É–ª–∏–∫–æ–≤—Å–∫–∞—è –±–∏—Ç–≤–∞?",
                "keywords": ["1380", "–∫—É–ª–∏–∫–æ–≤—Å–∫–∞—è", "–±–∏—Ç–≤–∞", "–¥–º–∏—Ç—Ä–∏–π"],
                "expected": "–ö—É–ª–∏–∫–æ–≤—Å–∫–∞—è –±–∏—Ç–≤–∞ –ø—Ä–æ–∏–∑–æ—à–ª–∞ –≤ 1380 –≥–æ–¥—É"
            },
            {
                "question": "–ö—Ç–æ –±—ã–ª –ø–µ—Ä–≤—ã–º —Ä—É—Å—Å–∫–∏–º —Ü–∞—Ä–µ–º?",
                "keywords": ["–∏–≤–∞–Ω", "–≥—Ä–æ–∑–Ω—ã–π", "—Ü–∞—Ä—å", "1547"],
                "expected": "–ò–≤–∞–Ω IV –ì—Ä–æ–∑–Ω—ã–π –±—ã–ª –ø–µ—Ä–≤—ã–º —Ä—É—Å—Å–∫–∏–º —Ü–∞—Ä–µ–º"
            },
            {
                "question": "–ö–æ–≥–¥–∞ –ø—Ä–æ–∏–∑–æ—à–ª–æ –∫—Ä–µ—â–µ–Ω–∏–µ –†—É—Å–∏?",
                "keywords": ["988", "–≤–ª–∞–¥–∏–º–∏—Ä", "–∫—Ä–µ—â–µ–Ω–∏–µ", "—Ö—Ä–∏—Å—Ç–∏–∞–Ω—Å—Ç–≤–æ"],
                "expected": "–ö—Ä–µ—â–µ–Ω–∏–µ –†—É—Å–∏ –ø—Ä–æ–∏–∑–æ—à–ª–æ –≤ 988 –≥–æ–¥—É"
            },
            {
                "question": "–ö–æ–≥–¥–∞ –Ω–∞—á–∞–ª–∞—Å—å –û—Ç–µ—á–µ—Å—Ç–≤–µ–Ω–Ω–∞—è –≤–æ–π–Ω–∞ 1812 –≥–æ–¥–∞?",
                "keywords": ["1812", "–Ω–∞–ø–æ–ª–µ–æ–Ω", "–æ—Ç–µ—á–µ—Å—Ç–≤–µ–Ω–Ω–∞—è", "–≤–æ–π–Ω–∞"],
                "expected": "–û—Ç–µ—á–µ—Å—Ç–≤–µ–Ω–Ω–∞—è –≤–æ–π–Ω–∞ 1812 –≥–æ–¥–∞ –Ω–∞—á–∞–ª–∞—Å—å –≤ 1812 –≥–æ–¥—É"
            },
            {
                "question": "–ö—Ç–æ –æ—Ç–º–µ–Ω–∏–ª –∫—Ä–µ–ø–æ—Å—Ç–Ω–æ–µ –ø—Ä–∞–≤–æ –≤ –†–æ—Å—Å–∏–∏?",
                "keywords": ["–∞–ª–µ–∫—Å–∞–Ω–¥—Ä", "–≤—Ç–æ—Ä–æ–π", "–∫—Ä–µ–ø–æ—Å—Ç–Ω–æ–µ", "–ø—Ä–∞–≤–æ", "1861"],
                "expected": "–ê–ª–µ–∫—Å–∞–Ω–¥—Ä II –æ—Ç–º–µ–Ω–∏–ª –∫—Ä–µ–ø–æ—Å—Ç–Ω–æ–µ –ø—Ä–∞–≤–æ –≤ 1861 –≥–æ–¥—É"
            }
        ]
        
        # –î–æ–±–∞–≤–ª—è–µ–º –±–∞–∑–æ–≤—ã–µ –≤–æ–ø—Ä–æ—Å—ã
        test_questions.extend(basic_questions)
        
        # –°–æ–∑–¥–∞–µ–º –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –≤–æ–ø—Ä–æ—Å—ã –Ω–∞ –æ—Å–Ω–æ–≤–µ –∑–∞–≥—Ä—É–∂–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
        for text in texts[:3]:  # –ë–µ—Ä–µ–º –ø–µ—Ä–≤—ã–µ 3 —Ç–µ–∫—Å—Ç–∞
            # –ò–∑–≤–ª–µ–∫–∞–µ–º –∫–ª—é—á–µ–≤—ã–µ –¥–∞—Ç—ã –∏ —Å–æ–±—ã—Ç–∏—è
            words = text.split()
            dates = [word for word in words if word.isdigit() and len(word) == 4 and 800 <= int(word) <= 2024]
            
            if dates:
                date = random.choice(dates)
                question = f"–ß—Ç–æ –ø—Ä–æ–∏—Å—Ö–æ–¥–∏–ª–æ –≤ {date} –≥–æ–¥—É?"
                test_questions.append({
                    "question": question,
                    "keywords": [date],
                    "expected": f"–°–æ–±—ã—Ç–∏—è {date} –≥–æ–¥–∞"
                })
        
        logger.info(f"–°–æ–∑–¥–∞–Ω–æ {len(test_questions)} —Ç–µ—Å—Ç–æ–≤—ã—Ö –≤–æ–ø—Ä–æ—Å–æ–≤")
        return test_questions
    
    def save_evaluation_report(self, results: Dict[str, Any], output_path: str):
        """
        –°–æ—Ö—Ä–∞–Ω—è–µ—Ç –æ—Ç—á–µ—Ç –æ–± –æ—Ü–µ–Ω–∫–µ –º–æ–¥–µ–ª–∏
        
        Args:
            results: –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è
            output_path: –ü—É—Ç—å –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –æ—Ç—á–µ—Ç–∞
        """
        try:
            # –°–æ–∑–¥–∞–µ–º –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é –µ—Å–ª–∏ –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç
            os.makedirs(os.path.dirname(output_path), exist_ok=True)
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ—Ç—á–µ—Ç
            with open(output_path, 'w', encoding='utf-8') as f:
                json.dump(results, f, ensure_ascii=False, indent=2)
            
            logger.info(f"–û—Ç—á–µ—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω –≤ {output_path}")
            
            # –í—ã–≤–æ–¥–∏–º –∫—Ä–∞—Ç–∫—É—é —Å–≤–æ–¥–∫—É
            print("\n" + "="*60)
            print("üìä –†–ï–ó–£–õ–¨–¢–ê–¢–´ –û–¶–ï–ù–ö–ò –ú–û–î–ï–õ–ò")
            print("="*60)
            print(f"üìù –í—Å–µ–≥–æ –≤–æ–ø—Ä–æ—Å–æ–≤: {results['total_questions']}")
            print(f"‚úÖ –ü—Ä–∞–≤–∏–ª—å–Ω—ã—Ö –æ—Ç–≤–µ—Ç–æ–≤: {results['correct_answers']} ({results['accuracy_percentage']:.1f}%)")
            print(f"‚ö†Ô∏è  –ß–∞—Å—Ç–∏—á–Ω–æ –ø—Ä–∞–≤–∏–ª—å–Ω—ã—Ö: {results['partial_answers']}")
            print(f"‚ùå –ù–µ–ø—Ä–∞–≤–∏–ª—å–Ω—ã—Ö –æ—Ç–≤–µ—Ç–æ–≤: {results['incorrect_answers']}")
            print(f"üéØ –û–±—â–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å: {results['partial_accuracy_percentage']:.1f}%")
            print("="*60)
            
            # –û—Ü–µ–Ω–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞ –æ–±—É—á–µ–Ω–∏—è
            if results['accuracy_percentage'] >= 70:
                print("üéâ –û–¢–õ–ò–ß–ù–û! –ú–æ–¥–µ–ª—å —Ö–æ—Ä–æ—à–æ –∏–∑—É—á–∏–ª–∞ –º–∞—Ç–µ—Ä–∏–∞–ª—ã")
            elif results['accuracy_percentage'] >= 50:
                print("üëç –•–û–†–û–®–û! –ú–æ–¥–µ–ª—å —á–∞—Å—Ç–∏—á–Ω–æ –∏–∑—É—á–∏–ª–∞ –º–∞—Ç–µ—Ä–∏–∞–ª—ã")
            elif results['accuracy_percentage'] >= 30:
                print("‚ö†Ô∏è  –£–î–û–í–õ–ï–¢–í–û–†–ò–¢–ï–õ–¨–ù–û! –ú–æ–¥–µ–ª—å —Å–ª–∞–±–æ –∏–∑—É—á–∏–ª–∞ –º–∞—Ç–µ—Ä–∏–∞–ª—ã")
            else:
                print("‚ùå –ü–õ–û–•–û! –ú–æ–¥–µ–ª—å –ø—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏ –Ω–µ –∏–∑—É—á–∏–ª–∞ –º–∞—Ç–µ—Ä–∏–∞–ª—ã")
            
            print("="*60)
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –æ—Ç—á–µ—Ç–∞: {e}")

def evaluate_model(model_path: str, data_path: str = "data/raw", model_type: str = "english"):
    """
    –û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –º–æ–¥–µ–ª–∏
    
    Args:
        model_path: –ü—É—Ç—å –∫ –æ–±—É—á–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏
        data_path: –ü—É—Ç—å –∫ –¥–∞–Ω–Ω—ã–º –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è —Ç–µ—Å—Ç–æ–≤—ã—Ö –≤–æ–ø—Ä–æ—Å–æ–≤
        model_type: –¢–∏–ø –º–æ–¥–µ–ª–∏
    """
    try:
        # –°–æ–∑–¥–∞–µ–º –æ—Ü–µ–Ω—â–∏–∫
        evaluator = ModelEvaluator(model_path, model_type)
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å
        if not evaluator.load_model():
            logger.error("–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –º–æ–¥–µ–ª—å –¥–ª—è –æ—Ü–µ–Ω–∫–∏")
            return False
        
        # –°–æ–∑–¥–∞–µ–º —Ç–µ—Å—Ç–æ–≤—ã–µ –≤–æ–ø—Ä–æ—Å—ã
        test_questions = evaluator.create_test_questions(data_path)
        if not test_questions:
            logger.error("–ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ–∑–¥–∞—Ç—å —Ç–µ—Å—Ç–æ–≤—ã–µ –≤–æ–ø—Ä–æ—Å—ã")
            return False
        
        # –ü—Ä–æ–≤–æ–¥–∏–º —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ
        results = evaluator.test_knowledge_retention(test_questions)
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ—Ç—á–µ—Ç
        output_path = f"evaluation_report_{model_type}.json"
        evaluator.save_evaluation_report(results, output_path)
        
        return True
        
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –æ—Ü–µ–Ω–∫–∏ –º–æ–¥–µ–ª–∏: {e}")
        return False

if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser(description="–û—Ü–µ–Ω–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞ –æ–±—É—á–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏")
    parser.add_argument("model_path", help="–ü—É—Ç—å –∫ –æ–±—É—á–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏")
    parser.add_argument("--data", default="data/raw", help="–ü—É—Ç—å –∫ –¥–∞–Ω–Ω—ã–º –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è —Ç–µ—Å—Ç–æ–≤")
    parser.add_argument("--type", default="english", choices=["english", "russian"], help="–¢–∏–ø –º–æ–¥–µ–ª–∏")
    
    args = parser.parse_args()
    
    success = evaluate_model(args.model_path, args.data, args.type)
    if success:
        print("‚úÖ –û—Ü–µ–Ω–∫–∞ –º–æ–¥–µ–ª–∏ –∑–∞–≤–µ—Ä—à–µ–Ω–∞ —É—Å–ø–µ—à–Ω–æ!")
    else:
        print("‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ—Ü–µ–Ω–∫–µ –º–æ–¥–µ–ª–∏!")
